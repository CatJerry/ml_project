{"cells":[{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["train Done.\n","test Done.\n"]}],"source":["import random\n","import os\n","import numpy as np\n","import pandas as pd\n","import gc\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n","from sklearn.metrics import accuracy_score,  precision_score, recall_score, make_scorer,f1_score\n","from xgboost import XGBClassifier\n","\n","def seed_everything(seed):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","\n","seed_everything(42) # Fixed Seed\n","\n","def csv_to_parquet(csv_path, save_name):\n","    df = pd.read_csv(csv_path)\n","    df.to_parquet(f'./{save_name}.parquet')\n","    del df\n","    gc.collect()\n","    print(save_name, 'Done.')\n","\n","csv_to_parquet('/Users/soyoung/ml_project/airplane/train.csv', 'train')\n","csv_to_parquet('/Users/soyoung/ml_project/airplane/test.csv', 'test')\n"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"GQRk7iXY2iYP"},"outputs":[{"name":"stdout","output_type":"stream","text":["Done.\n","Done.\n","Done.\n"]}],"source":["\n","train = pd.read_parquet('./train.parquet')\n","test = pd.read_parquet('./test.parquet')\n","sample_submission = pd.read_csv('/Users/soyoung/ml_project/airplane/sample_submission.csv', index_col = 0)\n","\n","airline_list = train[['Airline',\"Carrier_Code(IATA)\",\"Carrier_ID(DOT)\"]]\n","airline_list.dropna(inplace=True)\n","airline_list.drop_duplicates(inplace=True)\n","\n","def missing_values(df):\n","    missing_values = df[\"Carrier_ID(DOT)\"].isnull()\n","    for idx, value in enumerate(missing_values):\n","        if value:\n","            subset = airline_list[(airline_list[\"Airline\"] == df[\"Airline\"][idx]) | (airline_list[\"Carrier_Code(IATA)\"] == df[\"Carrier_Code(IATA)\"][idx])]\n","            if len(subset) > 0:\n","                df.at[idx, \"Carrier_ID(DOT)\"] = subset[\"Carrier_ID(DOT)\"].iloc[0]\n","                continue\n","    return df\n","\n","train = missing_values(train)\n","test = missing_values(test)\n","\n","# Replace variables with missing values except for the label (Delay) with the most frequent values of the training data\n","# 컬럼의 누락된 값은 훈련 데이터에서 해당 컬럼의 최빈값으로 대체됩니다.\n","NaN_col = ['Origin_State','Destination_State','Airline','Estimated_Departure_Time', 'Estimated_Arrival_Time','Carrier_Code(IATA)','Carrier_ID(DOT)']\n","\n","for col in NaN_col:\n","    mode = train[col].mode()[0]\n","    train[col] = train[col].fillna(mode)\n","    \n","    if col in test.columns:\n","        test[col] = test[col].fillna(mode)\n","print('Done.')\n","\n","# Quantify qualitative variables\n","# 정성적 변수는 LabelEncoder를 사용하여 숫자로 인코딩됩니다.\n","qual_col = ['Origin_Airport', 'Origin_State', 'Destination_Airport', 'Destination_State', 'Airline', 'Carrier_Code(IATA)', 'Tail_Number']\n","\n","for i in qual_col:\n","    le = LabelEncoder()\n","    le = le.fit(train[i])\n","    train[i] = le.transform(train[i])\n","    \n","    for label in np.unique(test[i]):\n","        if label not in le.classes_:\n","            le.classes_ = np.append(le.classes_, label)\n","    test[i] = le.transform(test[i])\n","print('Done.')\n","\n","# Remove unlabeled data\n","# 훈련 세트에서 레이블이 지정되지 않은 데이터가 제거되고 숫자 레이블 열이 추가됩니다.\n","train = train.dropna()\n","\n","column_number = {}\n","for i, column in enumerate(sample_submission.columns):\n","    column_number[column] = i\n","    \n","def to_number(x, dic):\n","    return dic[x]\n","\n","train.loc[:, 'Delay_num'] = train['Delay'].apply(lambda x: to_number(x, column_number))\n","print('Done.')\n","\n","train_x = train.drop(columns=['ID', 'Delay', 'Delay_num'])\n","train_y = train['Delay_num']\n","test_x = test.drop(columns=['ID'])\n","\n","# 교육 데이터는 교육 및 검증 세트로 분할되고 수치 기능은 StandardScaler를 사용하여 정규화됩니다.\n","# 모델은 GridSearchCV와 5겹 교차 검증을 사용하여 수행되는 하이퍼파라미터 튜닝과 함께 XGBClassifier를 사용하여 훈련됩니다.\n","# Split the training dataset into a training set and a validation set\n","train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size=0.2, random_state=42)\n","\n","# Normalize numerical features\n","scaler = StandardScaler()\n","train_x = scaler.fit_transform(train_x)\n","val_x = scaler.transform(val_x)\n","test_x = scaler.transform(test_x)\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting 5 folds for each of 12 candidates, totalling 60 fits\n","Accuracy: 0.8219838826689673\n","F1 Score: 0.744710236849881\n","Precision: 0.7814227640630016\n","Recall: 0.8219838826689673\n"]}],"source":["\n","# Cross-validation with StratifiedKFold\n","cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","\n","# Model and hyperparameter tuning using GridSearchCV\n","model = XGBClassifier(random_state=42)\n","\n","param_grid = {\n","    'learning_rate': [0.05,0.01],\n","    'max_depth': [3, 5, 7],\n","    'n_estimators': [100, 200],\n","}\n","\n","grid = GridSearchCV(model,\n","                    param_grid,\n","                    cv=cv,\n","                    scoring='accuracy',\n","                    n_jobs=-1,\n","                    verbose=1)\n","\n","grid.fit(train_x, train_y)\n","\n","best_model = grid.best_estimator_\n","\n","# Model evaluation\n","val_y_pred = best_model.predict(val_x)\n","accuracy = accuracy_score(val_y, val_y_pred)\n","f1 = f1_score(val_y, val_y_pred, average='weighted')\n","precision = precision_score(val_y, val_y_pred, average='weighted')\n","recall = recall_score(val_y, val_y_pred, average='weighted')\n","\n","print(f'Accuracy: {accuracy}')\n","print(f'F1 Score: {f1}')\n","print(f'Precision: {precision}')\n","print(f'Recall: {recall}')\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# 하이퍼파라미터 튜닝 결과를 바탕으로 최적의 모델을 선택하고 테스트 세트의 목표 변수를 예측하는 데 사용합니다.\n","# Model prediction\n","y_pred = best_model.predict_proba(test_x)\n","submission = pd.DataFrame(data=y_pred, columns=sample_submission.columns, index=sample_submission.index)\n","submission.to_csv('optimized_submission.csv', index=True)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["epoch 0  | loss: 0.85127 | val_0_unsup_loss_numpy: 0.7182000279426575|  0:04:42s\n","epoch 1  | loss: 0.8424  | val_0_unsup_loss_numpy: 0.7196599841117859|  0:09:33s\n","epoch 2  | loss: 0.84057 | val_0_unsup_loss_numpy: 0.7155100107192993|  0:14:21s\n","epoch 3  | loss: 0.83993 | val_0_unsup_loss_numpy: 0.6959499716758728|  0:19:08s\n","epoch 4  | loss: 0.83971 | val_0_unsup_loss_numpy: 0.6943699717521667|  0:23:49s\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[15], line 23\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39m# TabNetPretrainer\u001b[39;00m\n\u001b[1;32m     17\u001b[0m unsupervised_model \u001b[39m=\u001b[39m TabNetPretrainer(\n\u001b[1;32m     18\u001b[0m     optimizer_fn\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam,\n\u001b[1;32m     19\u001b[0m     optimizer_params\u001b[39m=\u001b[39m\u001b[39mdict\u001b[39m(lr\u001b[39m=\u001b[39m\u001b[39m2e-2\u001b[39m),\n\u001b[1;32m     20\u001b[0m     mask_type\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mentmax\u001b[39m\u001b[39m'\u001b[39m \u001b[39m# \"sparsemax\"\u001b[39;00m\n\u001b[1;32m     21\u001b[0m )\n\u001b[0;32m---> 23\u001b[0m unsupervised_model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     24\u001b[0m     X_train\u001b[39m=\u001b[39;49mtrain_x,\n\u001b[1;32m     25\u001b[0m     eval_set\u001b[39m=\u001b[39;49m[val_x],\n\u001b[1;32m     26\u001b[0m     pretraining_ratio\u001b[39m=\u001b[39;49m\u001b[39m0.8\u001b[39;49m,\n\u001b[1;32m     27\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     31\u001b[0m clf \u001b[39m=\u001b[39m TabNetClassifier(\n\u001b[1;32m     32\u001b[0m     optimizer_fn\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam,\n\u001b[1;32m     33\u001b[0m     optimizer_params\u001b[39m=\u001b[39m\u001b[39mdict\u001b[39m(lr\u001b[39m=\u001b[39m\u001b[39m2e-2\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m     mask_type\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msparsemax\u001b[39m\u001b[39m'\u001b[39m \u001b[39m# This will be overwritten if using pretrain model\u001b[39;00m\n\u001b[1;32m     39\u001b[0m )\n\u001b[1;32m     41\u001b[0m clf\u001b[39m.\u001b[39mfit(\n\u001b[1;32m     42\u001b[0m     X_train\u001b[39m=\u001b[39mtrain_x, y_train\u001b[39m=\u001b[39mtrain_y,\n\u001b[1;32m     43\u001b[0m     eval_set\u001b[39m=\u001b[39m[(train_x, train_y), (val_x, val_y)],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m     from_unsupervised\u001b[39m=\u001b[39munsupervised_model\n\u001b[1;32m     47\u001b[0m )\n","File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/downgrade/lib/python3.8/site-packages/pytorch_tabnet/pretraining.py:152\u001b[0m, in \u001b[0;36mTabNetPretrainer.fit\u001b[0;34m(self, X_train, eval_set, eval_name, loss_fn, pretraining_ratio, weights, max_epochs, patience, batch_size, virtual_batch_size, num_workers, drop_last, callbacks, pin_memory, warm_start)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39mfor\u001b[39;00m epoch_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_epochs):\n\u001b[1;32m    148\u001b[0m \n\u001b[1;32m    149\u001b[0m     \u001b[39m# Call method on_epoch_begin for all callbacks\u001b[39;00m\n\u001b[1;32m    150\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_container\u001b[39m.\u001b[39mon_epoch_begin(epoch_idx)\n\u001b[0;32m--> 152\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_epoch(train_dataloader)\n\u001b[1;32m    154\u001b[0m     \u001b[39m# Apply predict epoch to all eval sets\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     \u001b[39mfor\u001b[39;00m eval_name, valid_dataloader \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(eval_names, valid_dataloaders):\n","File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/downgrade/lib/python3.8/site-packages/pytorch_tabnet/pretraining.py:279\u001b[0m, in \u001b[0;36mTabNetPretrainer._train_epoch\u001b[0;34m(self, train_loader)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, X \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m    277\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_container\u001b[39m.\u001b[39mon_batch_begin(batch_idx)\n\u001b[0;32m--> 279\u001b[0m     batch_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_batch(X)\n\u001b[1;32m    281\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_container\u001b[39m.\u001b[39mon_batch_end(batch_idx, batch_logs)\n\u001b[1;32m    283\u001b[0m epoch_logs \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer\u001b[39m.\u001b[39mparam_groups[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m]}\n","File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/downgrade/lib/python3.8/site-packages/pytorch_tabnet/pretraining.py:315\u001b[0m, in \u001b[0;36mTabNetPretrainer._train_batch\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    312\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss(output, embedded_x, obf_vars)\n\u001b[1;32m    314\u001b[0m \u001b[39m# Perform backward pass and optimization\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    316\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclip_value:\n\u001b[1;32m    317\u001b[0m     clip_grad_norm_(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetwork\u001b[39m.\u001b[39mparameters(), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclip_value)\n","File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/downgrade/lib/python3.8/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n","File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/downgrade/lib/python3.8/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n","File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/downgrade/lib/python3.8/site-packages/torch/autograd/function.py:257\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mBackwardCFunction\u001b[39;00m(_C\u001b[39m.\u001b[39m_FunctionBase, FunctionCtx, _HookMixin):\n\u001b[0;32m--> 257\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs):\n\u001b[1;32m    258\u001b[0m         \u001b[39m# _forward_cls is defined by derived class\u001b[39;00m\n\u001b[1;32m    259\u001b[0m         \u001b[39m# The user should define either backward or vjp but never both.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m         backward_fn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_cls\u001b[39m.\u001b[39mbackward  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    261\u001b[0m         vjp_fn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_cls\u001b[39m.\u001b[39mvjp  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import torch\n","from pytorch_tabnet.pretraining import TabNetPretrainer\n","from pytorch_tabnet.tab_model import TabNetClassifier\n","from pytorch_tabnet.metrics import Metric\n","from sklearn.metrics import f1_score\n","\n","\n","class F1_Score(Metric):\n","    def __init__(self):\n","        self._name = \"f1\"\n","        self._maximize = True\n","\n","    def __call__(self, y_true, y_score):\n","        score = f1_score(y_true, (y_score[:, 1]>0.5)*1)\n","        return score\n","    \n","# TabNetPretrainer\n","unsupervised_model = TabNetPretrainer(\n","    optimizer_fn=torch.optim.Adam,\n","    optimizer_params=dict(lr=2e-2),\n","    mask_type='entmax' # \"sparsemax\"\n",")\n","\n","unsupervised_model.fit(\n","    X_train=train_x,\n","    eval_set=[val_x],\n","    pretraining_ratio=0.8,\n","    batch_size=16\n",")\n","\n","clf = TabNetClassifier(\n","    optimizer_fn=torch.optim.Adam,\n","    optimizer_params=dict(lr=2e-2),\n","\n","    # scheduler_params={\"step_size\":10, # how to use learning rate scheduler\n","    #                   \"gamma\":0.9},\n","    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n","    mask_type='sparsemax' # This will be overwritten if using pretrain model\n",")\n","\n","clf.fit(\n","    X_train=train_x, y_train=train_y,\n","    eval_set=[(train_x, train_y), (val_x, val_y)],\n","    eval_name=['train', 'valid'],\n","    eval_metric=['logloss','f1'],\n","    from_unsupervised=unsupervised_model\n",")\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOg97HOXQozdJlEBw+zTSTJ","mount_file_id":"1yqNKG2OnSgX9jMtReLQeGkNNlKOtMj9_","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"}},"nbformat":4,"nbformat_minor":0}
