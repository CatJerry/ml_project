{"cells":[{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["train Done.\n","test Done.\n"]}],"source":["import random\n","import os\n","import numpy as np\n","import pandas as pd\n","import gc\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n","from sklearn.metrics import accuracy_score,  precision_score, recall_score, make_scorer,f1_score\n","from xgboost import XGBClassifier\n","\n","def seed_everything(seed):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","\n","seed_everything(42) # Fixed Seed\n","\n","def csv_to_parquet(csv_path, save_name):\n","    df = pd.read_csv(csv_path)\n","    df.to_parquet(f'./{save_name}.parquet')\n","    del df\n","    gc.collect()\n","    print(save_name, 'Done.')\n","\n","csv_to_parquet('/Users/soyoung/ml_project/airplane/train.csv', 'train')\n","csv_to_parquet('/Users/soyoung/ml_project/airplane/test.csv', 'test')\n"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Done.\n"]}],"source":["train = pd.read_parquet('./train.parquet')\n","test = pd.read_parquet('./test.parquet')\n","sample_submission = pd.read_csv('/Users/soyoung/ml_project/airplane/sample_submission.csv', index_col = 0)\n","\n","airline_list = train[['Airline',\"Carrier_Code(IATA)\",\"Carrier_ID(DOT)\"]]\n","airline_list.dropna(inplace=True)\n","airline_list.drop_duplicates(inplace=True)\n","\n","def missing_values(df):\n","    missing_values = df[\"Carrier_ID(DOT)\"].isnull()\n","    for idx, value in enumerate(missing_values):\n","        if value:\n","            subset = airline_list[(airline_list[\"Airline\"] == df[\"Airline\"][idx]) | (airline_list[\"Carrier_Code(IATA)\"] == df[\"Carrier_Code(IATA)\"][idx])]\n","            if len(subset) > 0:\n","                df.at[idx, \"Carrier_ID(DOT)\"] = subset[\"Carrier_ID(DOT)\"].iloc[0]\n","                continue\n","    return df\n","\n","train = missing_values(train)\n","test = missing_values(test)\n","\n","# Replace variables with missing values except for the label (Delay) with the most frequent values of the training data\n","# 컬럼의 누락된 값은 훈련 데이터에서 해당 컬럼의 최빈값으로 대체됩니다.\n","NaN_col = ['Estimated_Departure_Time', 'Estimated_Arrival_Time']\n","\n","for col in NaN_col:\n","    mode = train[col].mode()[0]\n","    train[col] = train[col].fillna(mode)\n","    \n","    if col in test.columns:\n","        test[col] = test[col].fillna(mode)\n","print('Done.')\n","\n","\n"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[{"data":{"text/plain":["ID                               0\n","Month                            0\n","Day_of_Month                     0\n","Estimated_Departure_Time         0\n","Estimated_Arrival_Time           0\n","Cancelled                        0\n","Diverted                         0\n","Origin_Airport                   0\n","Origin_Airport_ID                0\n","Origin_State                109015\n","Destination_Airport              0\n","Destination_Airport_ID           0\n","Destination_State           109079\n","Distance                         0\n","Airline                     108920\n","Carrier_Code(IATA)          108990\n","Carrier_ID(DOT)               1273\n","Tail_Number                      0\n","Delay                       744999\n","dtype: int64"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["# \"Carrier_ID(DOT)\" 컬럼 값이 null인 행을 선택합니다.\n","train[train[\"Carrier_ID(DOT)\"].isnull()]\n","train.isnull().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["droplist_first = ['ID','Origin_State','Destination_State','Carrier_Code(IATA)','Airline']\n","train.drop(columns=droplist_first,inplace=True)\n","test.drop(columns=droplist_first,inplace=True)"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Done.\n"]}],"source":["\n","# Quantify qualitative variables\n","# 정성적 변수는 LabelEncoder를 사용하여 숫자로 인코딩됩니다.\n","qual_col = ['Origin_Airport', 'Destination_Airport', 'Tail_Number']\n","\n","for i in qual_col:\n","    le = LabelEncoder()\n","    le = le.fit(train[i])\n","    train[i] = le.transform(train[i])\n","    \n","    for label in np.unique(test[i]):\n","        if label not in le.classes_:\n","            le.classes_ = np.append(le.classes_, label)\n","    test[i] = le.transform(test[i])\n","print('Done.')"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["epoch 0  | loss: 192.40995| val_0_unsup_loss_numpy: 204.34649658203125|  0:00:41s\n","epoch 1  | loss: 149.485 | val_0_unsup_loss_numpy: 204.2398223876953|  0:01:21s\n","epoch 2  | loss: 108.30113| val_0_unsup_loss_numpy: 204.20974731445312|  0:02:02s\n","epoch 3  | loss: 72.64008| val_0_unsup_loss_numpy: 204.18020629882812|  0:02:43s\n","epoch 4  | loss: 45.32415| val_0_unsup_loss_numpy: 204.19676208496094|  0:03:23s\n","epoch 5  | loss: 27.28641| val_0_unsup_loss_numpy: 203.5570068359375|  0:04:04s\n","epoch 6  | loss: 15.73953| val_0_unsup_loss_numpy: 203.27999877929688|  0:04:46s\n","epoch 7  | loss: 9.3982  | val_0_unsup_loss_numpy: 203.13485717773438|  0:05:28s\n","epoch 8  | loss: 6.16895 | val_0_unsup_loss_numpy: 202.59197998046875|  0:06:09s\n","epoch 9  | loss: 4.69144 | val_0_unsup_loss_numpy: 202.02589416503906|  0:06:50s\n","epoch 10 | loss: 4.24826 | val_0_unsup_loss_numpy: 201.63180541992188|  0:07:31s\n","epoch 11 | loss: 3.70833 | val_0_unsup_loss_numpy: 201.35052490234375|  0:08:12s\n","epoch 12 | loss: 2.80764 | val_0_unsup_loss_numpy: 201.14793395996094|  0:08:54s\n","epoch 13 | loss: 2.5983  | val_0_unsup_loss_numpy: 107.9039535522461|  0:09:35s\n","epoch 14 | loss: 2.15004 | val_0_unsup_loss_numpy: 201.0256805419922|  0:10:16s\n","epoch 15 | loss: 1.9452  | val_0_unsup_loss_numpy: 201.2368927001953|  0:10:56s\n","epoch 16 | loss: 1.52393 | val_0_unsup_loss_numpy: 200.31028747558594|  0:11:37s\n","epoch 17 | loss: 1.32582 | val_0_unsup_loss_numpy: 200.3898468017578|  0:12:17s\n","epoch 18 | loss: 1.18414 | val_0_unsup_loss_numpy: 88.13641357421875|  0:12:58s\n","epoch 19 | loss: 1.39099 | val_0_unsup_loss_numpy: 41.57984161376953|  0:13:38s\n","epoch 20 | loss: 1.13072 | val_0_unsup_loss_numpy: 11.945670127868652|  0:14:19s\n","epoch 21 | loss: 1.03807 | val_0_unsup_loss_numpy: 8.947819709777832|  0:14:59s\n","epoch 22 | loss: 0.99038 | val_0_unsup_loss_numpy: 4.723559856414795|  0:15:40s\n","epoch 23 | loss: 1.20438 | val_0_unsup_loss_numpy: 3.02810001373291|  0:16:20s\n","epoch 24 | loss: 1.07562 | val_0_unsup_loss_numpy: 11.138859748840332|  0:17:00s\n","epoch 25 | loss: 0.97393 | val_0_unsup_loss_numpy: 23.815509796142578|  0:17:41s\n","epoch 26 | loss: 0.96014 | val_0_unsup_loss_numpy: 2.8401899337768555|  0:18:21s\n","epoch 27 | loss: 0.93849 | val_0_unsup_loss_numpy: 29.115890502929688|  0:19:02s\n","epoch 28 | loss: 1.09282 | val_0_unsup_loss_numpy: 10.991379737854004|  0:19:42s\n","epoch 29 | loss: 0.98566 | val_0_unsup_loss_numpy: 8.429089546203613|  0:20:23s\n","epoch 30 | loss: 0.95216 | val_0_unsup_loss_numpy: 3.3760199546813965|  0:21:03s\n","epoch 31 | loss: 0.93579 | val_0_unsup_loss_numpy: 2.1812100410461426|  0:21:43s\n","epoch 32 | loss: 1.01463 | val_0_unsup_loss_numpy: 2.5208001136779785|  0:22:24s\n","epoch 33 | loss: 1.11387 | val_0_unsup_loss_numpy: 1.3851900100708008|  0:23:04s\n","epoch 34 | loss: 0.96248 | val_0_unsup_loss_numpy: 1.3148599863052368|  0:23:44s\n","epoch 35 | loss: 0.93996 | val_0_unsup_loss_numpy: 1.152649998664856|  0:24:25s\n","epoch 36 | loss: 1.09348 | val_0_unsup_loss_numpy: 1.1588000059127808|  0:25:06s\n","epoch 37 | loss: 0.97956 | val_0_unsup_loss_numpy: 1.1150799989700317|  0:25:47s\n","epoch 38 | loss: 0.94697 | val_0_unsup_loss_numpy: 0.9923999905586243|  0:26:27s\n","epoch 39 | loss: 1.02991 | val_0_unsup_loss_numpy: 1.0839699506759644|  0:27:08s\n","epoch 40 | loss: 0.98272 | val_0_unsup_loss_numpy: 1.201550006866455|  0:27:49s\n","epoch 41 | loss: 0.98417 | val_0_unsup_loss_numpy: 1.364840030670166|  0:28:29s\n","epoch 42 | loss: 1.02138 | val_0_unsup_loss_numpy: 1.4326900243759155|  0:29:09s\n","epoch 43 | loss: 1.0119  | val_0_unsup_loss_numpy: 2.00219988822937|  0:29:49s\n","epoch 44 | loss: 0.95995 | val_0_unsup_loss_numpy: 2.4535999298095703|  0:30:29s\n","epoch 45 | loss: 0.93929 | val_0_unsup_loss_numpy: 1.7558499574661255|  0:31:09s\n","epoch 46 | loss: 1.05051 | val_0_unsup_loss_numpy: 1.473770022392273|  0:31:49s\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[48], line 25\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39m# 라벨이 있는 데이터셋으로 pretraining 모델 학습\u001b[39;00m\n\u001b[1;32m     19\u001b[0m unsupervised_model \u001b[39m=\u001b[39m TabNetPretrainer(\n\u001b[1;32m     20\u001b[0m     optimizer_fn\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam,\n\u001b[1;32m     21\u001b[0m     optimizer_params\u001b[39m=\u001b[39m\u001b[39mdict\u001b[39m(lr\u001b[39m=\u001b[39m\u001b[39m2e-2\u001b[39m),\n\u001b[1;32m     22\u001b[0m     mask_type\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mentmax\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     23\u001b[0m )\n\u001b[0;32m---> 25\u001b[0m unsupervised_model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     26\u001b[0m     X_train\u001b[39m=\u001b[39;49mlabeled_train_df\u001b[39m.\u001b[39;49mdrop(\u001b[39m'\u001b[39;49m\u001b[39mDelay\u001b[39;49m\u001b[39m'\u001b[39;49m, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49mvalues,\n\u001b[1;32m     27\u001b[0m     eval_set\u001b[39m=\u001b[39;49m[labeled_train_df\u001b[39m.\u001b[39;49mdrop(\u001b[39m'\u001b[39;49m\u001b[39mDelay\u001b[39;49m\u001b[39m'\u001b[39;49m, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49mvalues],\n\u001b[1;32m     28\u001b[0m     pretraining_ratio\u001b[39m=\u001b[39;49m\u001b[39m0.8\u001b[39;49m,\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     31\u001b[0m \u001b[39m# 라벨이 없는 데이터셋에서 예측값 생성\u001b[39;00m\n\u001b[1;32m     32\u001b[0m unlabeled_train_x \u001b[39m=\u001b[39m unlabeled_train_df\u001b[39m.\u001b[39mdrop(\u001b[39m'\u001b[39m\u001b[39mDelay\u001b[39m\u001b[39m'\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mvalues\n","File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/downgrade/lib/python3.8/site-packages/pytorch_tabnet/pretraining.py:152\u001b[0m, in \u001b[0;36mTabNetPretrainer.fit\u001b[0;34m(self, X_train, eval_set, eval_name, loss_fn, pretraining_ratio, weights, max_epochs, patience, batch_size, virtual_batch_size, num_workers, drop_last, callbacks, pin_memory, warm_start)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39mfor\u001b[39;00m epoch_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_epochs):\n\u001b[1;32m    148\u001b[0m \n\u001b[1;32m    149\u001b[0m     \u001b[39m# Call method on_epoch_begin for all callbacks\u001b[39;00m\n\u001b[1;32m    150\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_container\u001b[39m.\u001b[39mon_epoch_begin(epoch_idx)\n\u001b[0;32m--> 152\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_epoch(train_dataloader)\n\u001b[1;32m    154\u001b[0m     \u001b[39m# Apply predict epoch to all eval sets\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     \u001b[39mfor\u001b[39;00m eval_name, valid_dataloader \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(eval_names, valid_dataloaders):\n","File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/downgrade/lib/python3.8/site-packages/pytorch_tabnet/pretraining.py:279\u001b[0m, in \u001b[0;36mTabNetPretrainer._train_epoch\u001b[0;34m(self, train_loader)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, X \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m    277\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_container\u001b[39m.\u001b[39mon_batch_begin(batch_idx)\n\u001b[0;32m--> 279\u001b[0m     batch_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_batch(X)\n\u001b[1;32m    281\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_container\u001b[39m.\u001b[39mon_batch_end(batch_idx, batch_logs)\n\u001b[1;32m    283\u001b[0m epoch_logs \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer\u001b[39m.\u001b[39mparam_groups[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m]}\n","File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/downgrade/lib/python3.8/site-packages/pytorch_tabnet/pretraining.py:311\u001b[0m, in \u001b[0;36mTabNetPretrainer._train_batch\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[39mfor\u001b[39;00m param \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetwork\u001b[39m.\u001b[39mparameters():\n\u001b[1;32m    309\u001b[0m     param\u001b[39m.\u001b[39mgrad \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 311\u001b[0m output, embedded_x, obf_vars \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnetwork(X)\n\u001b[1;32m    312\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss(output, embedded_x, obf_vars)\n\u001b[1;32m    314\u001b[0m \u001b[39m# Perform backward pass and optimization\u001b[39;00m\n","File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/downgrade/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/downgrade/lib/python3.8/site-packages/pytorch_tabnet/tab_network.py:369\u001b[0m, in \u001b[0;36mTabNetPretraining.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[39m# set prior of encoder with obf_mask\u001b[39;00m\n\u001b[1;32m    368\u001b[0m prior \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m obf_vars\n\u001b[0;32m--> 369\u001b[0m steps_out, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(masked_x, prior\u001b[39m=\u001b[39;49mprior)\n\u001b[1;32m    370\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(steps_out)\n\u001b[1;32m    371\u001b[0m \u001b[39mreturn\u001b[39;00m res, embedded_x, obf_vars\n","File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/downgrade/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/downgrade/lib/python3.8/site-packages/pytorch_tabnet/tab_network.py:160\u001b[0m, in \u001b[0;36mTabNetEncoder.forward\u001b[0;34m(self, x, prior)\u001b[0m\n\u001b[1;32m    158\u001b[0m steps_output \u001b[39m=\u001b[39m []\n\u001b[1;32m    159\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_steps):\n\u001b[0;32m--> 160\u001b[0m     M \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49matt_transformers[step](prior, att)\n\u001b[1;32m    161\u001b[0m     M_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(\n\u001b[1;32m    162\u001b[0m         torch\u001b[39m.\u001b[39msum(torch\u001b[39m.\u001b[39mmul(M, torch\u001b[39m.\u001b[39mlog(M \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepsilon)), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    164\u001b[0m     \u001b[39m# update prior\u001b[39;00m\n","File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/downgrade/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/downgrade/lib/python3.8/site-packages/pytorch_tabnet/tab_network.py:638\u001b[0m, in \u001b[0;36mAttentiveTransformer.forward\u001b[0;34m(self, priors, processed_feat)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, priors, processed_feat):\n\u001b[1;32m    637\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(processed_feat)\n\u001b[0;32m--> 638\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbn(x)\n\u001b[1;32m    639\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmul(x, priors)\n\u001b[1;32m    640\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mselector(x)\n","File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/downgrade/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/downgrade/lib/python3.8/site-packages/pytorch_tabnet/tab_network.py:36\u001b[0m, in \u001b[0;36mGBN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     35\u001b[0m     chunks \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mchunk(\u001b[39mint\u001b[39m(np\u001b[39m.\u001b[39mceil(x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvirtual_batch_size)), \u001b[39m0\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m     res \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn(x_) \u001b[39mfor\u001b[39;00m x_ \u001b[39min\u001b[39;00m chunks]\n\u001b[1;32m     38\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(res, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n","File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/downgrade/lib/python3.8/site-packages/pytorch_tabnet/tab_network.py:36\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     35\u001b[0m     chunks \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mchunk(\u001b[39mint\u001b[39m(np\u001b[39m.\u001b[39mceil(x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvirtual_batch_size)), \u001b[39m0\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m     res \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbn(x_) \u001b[39mfor\u001b[39;00m x_ \u001b[39min\u001b[39;00m chunks]\n\u001b[1;32m     38\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(res, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n","File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/downgrade/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/downgrade/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m    172\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    173\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[1;32m    175\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[1;32m    176\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    177\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    178\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m    180\u001b[0m     bn_training,\n\u001b[1;32m    181\u001b[0m     exponential_average_factor,\n\u001b[1;32m    182\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[1;32m    183\u001b[0m )\n","File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/downgrade/lib/python3.8/site-packages/torch/nn/functional.py:2450\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2447\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[1;32m   2448\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[0;32m-> 2450\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m   2451\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled\n\u001b[1;32m   2452\u001b[0m )\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import pandas as pd\n","import numpy as np\n","import torch\n","from pytorch_tabnet.pretraining import TabNetPretrainer\n","from pytorch_tabnet.tab_model import TabNetClassifier\n","from pytorch_tabnet.metrics import Metric\n","from sklearn.metrics import f1_score\n","train.dropna(inplace=True)\n","train.replace('Delayed',1,inplace=True)\n","train.replace('Not_Delayed',0,inplace=True)\n","\n","# 라벨이 있는 데이터셋\n","labeled_train_df = train.dropna(subset=['Delay'])\n","\n","# 라벨이 없는 데이터셋\n","unlabeled_train_df = train[train['Delay'].isna()]\n","\n","# 라벨이 있는 데이터셋으로 pretraining 모델 학습\n","unsupervised_model = TabNetPretrainer(\n","    optimizer_fn=torch.optim.Adam,\n","    optimizer_params=dict(lr=2e-2),\n","    mask_type='entmax'\n",")\n","\n","unsupervised_model.fit(\n","    X_train=labeled_train_df.drop('Delay', axis=1).values,\n","    eval_set=[labeled_train_df.drop('Delay', axis=1).values],\n","    pretraining_ratio=0.8,\n",")\n","\n","# 라벨이 없는 데이터셋에서 예측값 생성\n","unlabeled_train_x = unlabeled_train_df.drop('Delay', axis=1).values\n","unlabeled_pred = unsupervised_model.predict(unlabeled_train_x)\n","unlabeled_train_x = np.hstack((unlabeled_train_x, unlabeled_pred))\n","\n","# 라벨이 있는 데이터셋\n","labeled_train_x = labeled_train_df.drop('Delay', axis=1).values\n","labeled_train_y = labeled_train_df['Delay'].values\n","\n","# 라벨이 결측된 데이터 제외한 라벨이 없는 데이터셋\n","unlabeled_train_x = unlabeled_train_x[~np.isnan(unlabeled_train_df['Delay'])]\n","\n","# 라벨이 결측된 데이터 제외한 라벨이 있는 데이터셋\n","labeled_train_x = labeled_train_x[~np.isnan(labeled_train_y)]\n","labeled_train_y = labeled_train_y[~np.isnan(labeled_train_y)]\n","\n","# Fine-tuning을 위해 새로운 feature를 추가한 train_x, train_y 생성\n","train_x = np.vstack((labeled_train_x, unlabeled_train_x))\n","train_y = np.concatenate((labeled_train_y, unlabeled_train_df['Delay'].values))\n","\n","# TabNetClassifier 모델 학습\n","clf = TabNetClassifier(\n","    optimizer_fn=torch.optim.Adam,\n","    optimizer_params=dict(lr=2e-2),\n","    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n","    mask_type='sparsemax'\n","    \n",")\n","from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n","\n","# EarlyStopping 객체 생성\n","early_stop_callback = EarlyStopping(\n","    monitor='logloss', # 모니터링할 metric 선택\n","    min_delta=0.1, # metric 개선 여부를 판단하는 최소 차이 값\n","    patience=10, # 개선이 없을 경우 학습을 중지하기까지의 epoch 수\n","    verbose=False, # True일 경우 early stopping이 실행될 때마다 메시지를 출력합니다.\n","    mode='min' # 모니터링할 metric이 감소해야 하는지, 증가해야 하는지를 선택합니다.\n",")\n","\n","# val_0_unsup_loss_numpy는 unsupervised pretraining 단계에서 계산된 unsupervised loss 값을 나타냅니다. 크다고 나쁜거 아님.\n","clf.fit(\n","    X_train=train_x,\n","    y_train=train_y,\n","    eval_set=[(train_x, train_y)],\n","    eval_name=['train'],\n","    eval_metric=['logloss','f1'],\n","    from_unsupervised=unsupervised_model,\n","    callbacks=[early_stop_callback] # EarlyStopping 객체 전달\n","\n",")\n"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'clf' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m y_pred \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mpredict_proba(test)\n","\u001b[0;31mNameError\u001b[0m: name 'clf' is not defined"]}],"source":["y_pred = clf.predict_proba(test)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["submission = pd.DataFrame(data=y_pred, columns=sample_submission.columns, index=sample_submission.index)\n","submission.to_csv('submission_tabnet.csv', index=True)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOg97HOXQozdJlEBw+zTSTJ","mount_file_id":"1yqNKG2OnSgX9jMtReLQeGkNNlKOtMj9_","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"}},"nbformat":4,"nbformat_minor":0}
